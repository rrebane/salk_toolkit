{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I/O\n",
    "> Functions to handle reading and writing datasets and model descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "import json, os\n",
    "import itertools as it\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "from typing import List, Tuple, Dict, Union, Optional\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "import salk_toolkit as stk\n",
    "from salk_toolkit.utils import replace_constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Default usage with mature metafile: read_annotated_data(<metafile name>)\n",
    "# When figuring out the metafile, it can also be run as: read_annotated_data(meta=<dict>, data_file=<>)\n",
    "def read_annotated_data(meta_fname=None, multilevel=False, meta=None, data_file=None, return_meta=False):\n",
    "    \n",
    "    # Read metafile\n",
    "    if meta_fname:\n",
    "        with open(meta_fname,'r') as jf:\n",
    "            meta = json.load(jf)\n",
    "    \n",
    "    # Setup constants with a simple replacement mechanic\n",
    "    replace_constants(meta)\n",
    "    \n",
    "    # Read datafile\n",
    "    if not data_file:\n",
    "        data_file = os.path.join(os.path.dirname(meta_fname),meta['file'])\n",
    "    opts = meta['read_opts'] if'read_opts' in meta else {}\n",
    "    raw_data = pd.read_csv(data_file, **opts)\n",
    "    \n",
    "    res = []\n",
    "    \n",
    "\n",
    "    \n",
    "    if 'preprocessing' in meta:\n",
    "        exec(meta['preprocessing'],{'pd':pd, 'np':np },{ 'df':raw_data })\n",
    "        \n",
    "    for group in meta['structure']:\n",
    "        gres = []\n",
    "        for tpl in group['columns']:\n",
    "            if type(tpl)==list:\n",
    "                cn = tpl[0] # column name\n",
    "                sn = tpl[1] if type(tpl[1])==str else cn # source column\n",
    "                cd = tpl[2] if len(tpl)==3 else tpl[1] if type(tpl[1])==dict else {} # metadata\n",
    "            else:\n",
    "                cn = sn = tpl\n",
    "                cd = {}\n",
    "\n",
    "            if 'scale' in group: cd = {**group['scale'],**cd}\n",
    "            if sn not in raw_data:\n",
    "                print(f\"Column {sn} not found\")\n",
    "                continue\n",
    "            s = raw_data[sn].rename(cn)\n",
    "            \n",
    "            if 'translate' in cd: s.replace(cd['translate'],inplace=True)\n",
    "            \n",
    "            if 'transform' in cd: s = eval(cd['transform'],{ 's':s, 'df':raw_data, 'pd':pd, 'np':np })\n",
    "            \n",
    "            if 'categories' in cd: \n",
    "                na_sum = s.isna().sum()\n",
    "                cats = cd['categories'] if cd['categories']!='infer' else [ c for c in s.unique() if pd.notna(c) ]\n",
    "                s = pd.Series(pd.Categorical(s,categories=cats,ordered=cd['ordered'] if 'ordered' in cd else False), name=cn)\n",
    "                # Check if the category list provided was comprehensive\n",
    "                new_nas = s.isna().sum() - na_sum\n",
    "                if new_nas > 0: print(f'Column {cn} has {new_nas} entries that were not listed in categories')\n",
    "            gres.append(s)\n",
    "        if len(gres)==0: continue\n",
    "        gdf = pd.concat(gres,axis=1)\n",
    "        gdf.columns = pd.MultiIndex.from_arrays([[group['name']]*len(gdf.columns),gdf.columns])\n",
    "        res.append(gdf)\n",
    "    \n",
    "    df = pd.concat(res,axis=1)\n",
    "    \n",
    "    if 'postprocessing' in meta:\n",
    "        exec(meta['postprocessing'],{'pd':pd, 'np':np },{ 'df': df })\n",
    "\n",
    "    if not multilevel:\n",
    "        df.columns = df.columns.get_level_values(1)    \n",
    "    \n",
    "    return (df, meta) if return_meta else df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def read_and_process_data(desc, return_meta=False):\n",
    "    data, meta = read_annotated_data(desc['file'],return_meta=True)\n",
    "    \n",
    "    # Perform transformation and filtering\n",
    "    if 'preprocessing' in desc: exec(desc['preprocessing'],  {'pd':pd, 'np':np },{ 'df':data })\n",
    "    if 'filter' in desc: data = data[eval(desc['filter'],    {'pd':pd, 'np':np },{ 'df':data })]\n",
    "    if 'postprocessing' in desc: exec(desc['postprocessing'],{'pd':pd, 'np':np },{ 'df':data })\n",
    "    \n",
    "    return (data, meta) if return_meta else data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/master_meta.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_293761/1283506962.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m }\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_and_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataf_meta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_293761/3464427873.py\u001b[0m in \u001b[0;36mread_and_process_data\u001b[0;34m(desc, return_meta)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#| export\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_and_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdesc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_meta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_annotated_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdesc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'file'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreturn_meta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Perform transformation and filtering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_293761/223167203.py\u001b[0m in \u001b[0;36mread_annotated_data\u001b[0;34m(meta_fname, multilevel, meta, data_file, return_meta)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Read metafile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmeta_fname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_fname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mjf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0mmeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/master_meta.json'"
     ]
    }
   ],
   "source": [
    "dataf_meta = {\n",
    "    'file': '../data/master_meta.json',\n",
    "    'preprocessing': \"df.age_group.replace({'16-24':'18-24'}, inplace=True)\",\n",
    "    'filter': '(df.citizen) & (df.age>=18) & (df.wave>df.wave.max()-5)',\n",
    "}\n",
    "\n",
    "df = read_and_process_data(dataf_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def save_population_h5(fname,pdf):\n",
    "    hdf = pd.HDFStore(fname,complevel=9, complib='zlib')\n",
    "    hdf.put('population',pdf,format='table')\n",
    "    hdf.close()\n",
    "    \n",
    "def load_population_h5(fname):\n",
    "    hdf =  pd.HDFStore(fname, mode='r')\n",
    "    res = hdf['population'].copy()\n",
    "    hdf.close()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def save_sample_h5(fname,trace,COORDS = None, filter_df = None):\n",
    "    odims = [d for d in trace.predictions.dims if d not in ['chain','draw','obs_idx']]\n",
    "    \n",
    "    if COORDS is None: # Recover them from trace (requires posterior be saved in same trace)\n",
    "        inds = trace.posterior.indexes\n",
    "        coords = { t: list(inds[t]) for t in inds if t not in ['chain','draw'] and '_dim_' not in t}\n",
    "        COORDS = { 'immutable': coords, 'mutable': ['obs_idx'] }\n",
    "\n",
    "    if filter_df is None: # Recover filter dimensions and data from trace (works only for GLMs)\n",
    "        rmdims = odims + list({'time','unit','combined_inputs'} & set(trace.predictions_constant_data.dims))\n",
    "        df = trace.predictions_constant_data.drop_dims(rmdims).to_dataframe()#.set_index(demographics_order).indexb\n",
    "        df.columns = [ s.removesuffix('_id') for s in df.columns]\n",
    "        df.drop(columns=[c for c in df.columns if c[:4]=='obs_'],inplace=True)\n",
    "\n",
    "        for d in df.columns:\n",
    "            if d in COORDS['immutable']:\n",
    "                fs = COORDS['immutable'][d]\n",
    "                df[d] = pd.Categorical(df[d].replace(dict(enumerate(fs))),fs)\n",
    "                if d in orders: df[d] = pd.Categorical(df[d],orders[d],ordered=True)\n",
    "        filter_df = df\n",
    "\n",
    "    chains, draws = trace.predictions.dims['chain'], trace.predictions.dims['draw']\n",
    "    dinds = np.array(list(it.product( range(chains), range(draws), list(filter_df.index)))).reshape( (-1, 3) )\n",
    "\n",
    "    res_dfs = { 'filter': filter_df }\n",
    "    for odim in odims:\n",
    "        response_cols = list(np.array(trace.predictions[odim]))\n",
    "        xdf = pd.DataFrame(np.concatenate( (\n",
    "            dinds,\n",
    "            np.array(trace.predictions['y_'+odim]).reshape( ( -1,len(response_cols) ) )\n",
    "            ), axis=-1), columns = ['chain', 'draw', 'obs_idx'] + response_cols)\n",
    "        res_dfs[odim] = postprocess_rdf(xdf,odim)\n",
    "        \n",
    "    # Save dfs as hdf5\n",
    "    hdf = pd.HDFStore(fname,complevel=9, complib='zlib')\n",
    "    for k,vdf in res_dfs.items():\n",
    "        hdf.put(k,vdf,format='table')\n",
    "    hdf.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# These two very helpful functions are borrowed from https://towardsdatascience.com/saving-metadata-with-dataframes-71f51f558d8e\n",
    "\n",
    "custom_meta_key = 'salk-toolkit-meta'\n",
    "\n",
    "def save_parquet_with_metadata(df, meta, file_name):\n",
    "    table = pa.Table.from_pandas(df)\n",
    "    \n",
    "    custom_meta_json = json.dumps(meta)\n",
    "    existing_meta = table.schema.metadata\n",
    "    combined_meta = {\n",
    "        custom_meta_key.encode() : custom_meta_json.encode(),\n",
    "        **existing_meta\n",
    "    }\n",
    "    table = table.replace_schema_metadata(combined_meta)\n",
    "    \n",
    "    pq.write_table(table, file_name, compression='GZIP')\n",
    "    \n",
    "def load_parquet_with_metadata(file_name):\n",
    "    restored_table = pq.read_table(file_name)\n",
    "    restored_df = restored_table.to_pandas()\n",
    "    restored_meta_json = restored_table.schema.metadata[custom_meta_key.encode()]\n",
    "    restored_meta = json.loads(restored_meta_json)\n",
    "    \n",
    "    return restored_df, restored_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test saving and loading parquet with metadata\n",
    "df = pd.DataFrame([[1,2],[3,4]],columns=['x','y'])\n",
    "meta = { 'test': [1,{'x':2},[3]] }\n",
    "\n",
    "save_parquet_with_metadata(df,meta,'test.parquet')\n",
    "ndf, nmeta = load_parquet_with_metadata('test.parquet')\n",
    "\n",
    "assert nmeta == meta\n",
    "assert ndf.equals(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is how you can add tests\n",
    "#assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
