{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I/O\n",
    "> Functions to handle reading and writing datasets and model descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "import json, os\n",
    "import itertools as it\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "from typing import List, Tuple, Dict, Union, Optional\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pyreadstat\n",
    "\n",
    "import salk_toolkit as stk\n",
    "from salk_toolkit.utils import replace_constants, vod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def read_json(fname,replace_const=True):\n",
    "    with open(fname,'r') as jf:\n",
    "        meta = json.load(jf)\n",
    "    if replace_const:\n",
    "        meta = replace_constants(meta)\n",
    "    return meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Default usage with mature metafile: read_annotated_data(<metafile name>)\n",
    "# When figuring out the metafile, it can also be run as: read_annotated_data(meta=<dict>, data_file=<>)\n",
    "def read_annotated_data(meta_fname=None, multilevel=False, meta=None, data_file=None, return_meta=False, only_fix_categories=False):\n",
    "    \n",
    "    # Read metafile\n",
    "    if meta_fname:\n",
    "        meta = read_json(meta_fname,replace_const=False)\n",
    "    \n",
    "    # Setup constants with a simple replacement mechanic\n",
    "    constants = meta['constants'] if 'constants' in meta else {}\n",
    "    meta = replace_constants(meta)\n",
    "    \n",
    "    # Read datafile\n",
    "    if not data_file:\n",
    "        data_file = os.path.join(os.path.dirname(meta_fname),meta['file'])\n",
    "    opts = meta['read_opts'] if'read_opts' in meta else {}\n",
    "    \n",
    "    if data_file[-3:] == 'csv':\n",
    "        raw_data = pd.read_csv(data_file, **opts)\n",
    "    elif data_file[-3:] == 'sav':\n",
    "        raw_data, _ = pyreadstat.read_sav(data_file, **{ 'apply_value_formats':True, 'dates_as_pandas_datetime':True },**opts)\n",
    "    elif data_file[-7:] == 'parquet':\n",
    "        raw_data = pd.read_parquet(data_file, **opts)\n",
    "    else:\n",
    "        raise Exception(f\"Not a known file format {data_file}\")\n",
    "    \n",
    "    res = []\n",
    "    \n",
    "    if 'preprocessing' in meta and not only_fix_categories:\n",
    "        exec(meta['preprocessing'],{'pd':pd, 'np':np, 'stk':stk, 'df':raw_data, **constants })\n",
    "        \n",
    "    for group in meta['structure']:\n",
    "        gres = []\n",
    "        for tpl in group['columns']:\n",
    "            if type(tpl)==list:\n",
    "                cn = tpl[0] # column name\n",
    "                sn = tpl[1] if type(tpl[1])==str else cn # source column\n",
    "                cd = tpl[2] if len(tpl)==3 else tpl[1] if type(tpl[1])==dict else {} # metadata\n",
    "            else:\n",
    "                cn = sn = tpl\n",
    "                cd = {}\n",
    "                \n",
    "            if only_fix_categories: sn = cn\n",
    "\n",
    "            if 'scale' in group: cd = {**group['scale'],**cd}\n",
    "            \n",
    "            if sn not in raw_data:\n",
    "                print(f\"Column {sn} not found\")\n",
    "                continue\n",
    "            \n",
    "            if raw_data[sn].isna().all():\n",
    "                print(f\"Column {sn} is empty and thus ignored\")\n",
    "                continue\n",
    "                \n",
    "            s = raw_data[sn].rename(cn)\n",
    "            \n",
    "            if not only_fix_categories:\n",
    "                if 'translate' in cd: s.replace(cd['translate'],inplace=True)\n",
    "                if 'transform' in cd: s = eval(cd['transform'],{ 's':s, 'df':raw_data, 'pd':pd, 'np':np, 'stk':stk , **constants })\n",
    "\n",
    "            if 'categories' in cd: \n",
    "                na_sum = s.isna().sum()\n",
    "                cats = cd['categories'] if cd['categories']!='infer' else [ c for c in s.unique() if pd.notna(c) ]\n",
    "                s = pd.Series(pd.Categorical(s,categories=cats,ordered=cd['ordered'] if 'ordered' in cd else False), name=cn)\n",
    "                # Check if the category list provided was comprehensive\n",
    "                new_nas = s.isna().sum() - na_sum\n",
    "                if new_nas > 0: print(f'Column {cn} has {new_nas} entries that were not listed in categories')\n",
    "            gres.append(s)\n",
    "        if len(gres)==0: continue\n",
    "        gdf = pd.concat(gres,axis=1)\n",
    "        gdf.columns = pd.MultiIndex.from_arrays([[group['name']]*len(gdf.columns),gdf.columns])\n",
    "        res.append(gdf)\n",
    "    \n",
    "    df = pd.concat(res,axis=1)\n",
    "    \n",
    "    if 'postprocessing' in meta and not only_fix_categories:\n",
    "        exec(meta['postprocessing'],{'pd':pd, 'np':np, 'stk':stk, 'df':df, **constants  })\n",
    "\n",
    "    if not multilevel:\n",
    "        df.columns = df.columns.get_level_values(1)    \n",
    "    \n",
    "    return (df, meta) if return_meta else df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Helper functions designed to be used with the annotations\n",
    "\n",
    "# Convert data_meta into a dict where each group and column maps to their metadata dict\n",
    "def extract_column_meta(data_meta):\n",
    "    res = defaultdict(lambda: {})\n",
    "    for g in data_meta['structure']:\n",
    "        base = g['scale'] if 'scale' in g else {}\n",
    "        res[g['name']] = base\n",
    "        for cd in g['columns']:\n",
    "            if isinstance(cd,str): cd = [cd]\n",
    "            res[cd[0]] = {**base,**cd[-1]} if isinstance(cd[-1],dict) else base\n",
    "    return res\n",
    "\n",
    "# Convert data_meta into a dict of group_name -> [column names]\n",
    "def group_columns_dict(data_meta):\n",
    "    return { g['name'] : [(t[0] if type(t)!=str else t) for t in g['columns']] for g in data_meta['structure'] }\n",
    "\n",
    "# Take a list and a dict and replace all dict keys in list with their corresponding lists in-place\n",
    "def list_aliases(lst, da):\n",
    "    return [ fv for v in lst for fv in (da[v] if v in da else [v]) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert list_aliases(['a','b','c'],{'b': ['x','y']}) == ['a', 'x', 'y', 'c']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "# Creates a mapping old -> new\n",
    "def get_original_column_names(dmeta):\n",
    "    res = {}\n",
    "    for g in dmeta['structure']:\n",
    "        for c in g['columns']:\n",
    "            if isinstance(c,str): res[c] = c\n",
    "            if len(c)==1: res[c[0]] = c[0]\n",
    "            elif len(c)>=2 and isinstance(c[1],str): res[c[1]] = c[0]\n",
    "    return res\n",
    "\n",
    "# Map ot backwards and nt forwards to move from one to the other\n",
    "def change_mapping(ot, nt, only_matches=False):\n",
    "    # Todo: warn about non-bijective mappings\n",
    "    matches = { v: nt[k] for k, v in ot.items() if k in nt and v!=nt[k] } # change those that are shared\n",
    "    if only_matches: return matches\n",
    "    else: \n",
    "        return { **{ v:k for k, v in ot.items() if k not in nt }, # undo those in ot not in nt\n",
    "                 **{ k:v for k, v in nt.items() if k not in ot }, # do those in nt not in ot\n",
    "                 **matches } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Change an existing dataset to correspond better to a new meta_data\n",
    "# This is intended to allow making small improvements in the meta even after a model has been run\n",
    "# It is by no means perfect, but is nevertheless a useful tool to avoid re-running long pymc models for simple column/translation changes\n",
    "def change_meta_df(df, old_dmeta, new_dmeta):\n",
    "    print(\"Warning: this tool handles only simple cases of column name, translation and category order changes.\")\n",
    "    \n",
    "    # Ready the metafiles for parsing\n",
    "    old_dmeta = replace_constants(old_dmeta); new_dmeta = replace_constants(new_dmeta)\n",
    "    \n",
    "    # Rename columns \n",
    "    ocn, ncn = get_original_column_names(old_dmeta), get_original_column_names(new_dmeta)\n",
    "    name_changes = change_mapping(ocn,ncn,only_matches=True)\n",
    "    if name_changes != {}: print(f\"Renaming columns: {name_changes}\")\n",
    "    df.rename(columns=name_changes,inplace=True)\n",
    "    \n",
    "    rev_name_changes = { v: k for k,v in name_changes.items() }\n",
    "    \n",
    "    # Get metadata for each column\n",
    "    ocm = extract_column_meta(old_dmeta)\n",
    "    ncm = extract_column_meta(new_dmeta)\n",
    "    \n",
    "    for c in ncm.keys():\n",
    "        if c not in df.columns: continue # probably group\n",
    "        if c not in ocm.keys(): continue # new column\n",
    "        \n",
    "        ncd, ocd = ncm[c], ocm[rev_name_changes[c] if c in rev_name_changes else c]\n",
    "        \n",
    "        # Warn about transformations and don't touch columns where those change\n",
    "        if vod(ocd,'transform') != vod(ncd,'transform'):\n",
    "            print(f\"Warning: column {c} has a different transformation. Leaving it unchanged\")\n",
    "            continue\n",
    "        \n",
    "        # Handle translation changes\n",
    "        ot, nt = vod(ocd,'translate',{}), vod(ncd,'translate',{})\n",
    "        remap = change_mapping(ot,nt)\n",
    "        if remap != {}: print(f\"Remapping {c} with {remap}\")\n",
    "        df[c].replace(remap,inplace=True)\n",
    "        \n",
    "        # Reorder categories and/or change ordered status\n",
    "        if vod(ocd,'categories') != vod(ncd,'categories') or vod(ocd,'ordered') != vod(ncd,'ordered'):\n",
    "            cats = vod(ncd,'categories')\n",
    "            if isinstance(cats,list):\n",
    "                print(f\"Changing {c} to Cat({cats},ordered={vod(ncd,'ordered')}\")\n",
    "                df[c] = pd.Categorical(df[c],categories=cats,ordered=vod(ncd,'ordered'))\n",
    "    \n",
    "    # column order changes\n",
    "    gcdict = group_columns_dict(new_dmeta)\n",
    "    \n",
    "    cols = ['draw','obs_idx'] + [ c for g in new_dmeta['structure'] for c in gcdict[g['name']]]\n",
    "    cols = [ c for c in cols if c in df.columns ]\n",
    "    \n",
    "    return df[cols]\n",
    "\n",
    "def change_parquet_meta(orig_file,data_metafile,new_file):\n",
    "    df, meta = load_parquet_with_metadata(orig_file)\n",
    "    \n",
    "    new_data_meta = read_json(data_metafile, replace_const=True)\n",
    "    df = change_meta_df(df,meta['data'],new_data_meta)\n",
    "    \n",
    "    meta['old_data'] = meta['data']\n",
    "    meta['data'] = new_data_meta\n",
    "    save_parquet_with_metadata(df,meta,new_file)\n",
    "    \n",
    "    return df, meta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def read_and_process_data(desc, return_meta=False):\n",
    "    data, meta = read_annotated_data(desc['file'],return_meta=True)\n",
    "    \n",
    "    # Perform transformation and filtering\n",
    "    if 'preprocessing' in desc: exec(desc['preprocessing'],  {'pd':pd, 'np':np },{ 'df':data })\n",
    "    if 'filter' in desc: data = data[eval(desc['filter'],    {'pd':pd, 'np':np },{ 'df':data })]\n",
    "    if 'postprocessing' in desc: exec(desc['postprocessing'],{'pd':pd, 'np':np },{ 'df':data })\n",
    "    \n",
    "    return (data, meta) if return_meta else data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_430929/428843233.py:21: DtypeWarning: Columns (42) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  raw_data = pd.read_csv(data_file, **opts)\n"
     ]
    }
   ],
   "source": [
    "dataf_meta = {\n",
    "    'file': '../data/master_meta.json',\n",
    "    'preprocessing': \"df.age_group.replace({'16-24':'18-24'}, inplace=True)\",\n",
    "    'filter': '(df.citizen) & (df.age>=18) & (df.wave<5)',\n",
    "}\n",
    "\n",
    "df = read_and_process_data(dataf_meta)\n",
    "assert len(df) == 4355"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def save_population_h5(fname,pdf):\n",
    "    hdf = pd.HDFStore(fname,complevel=9, complib='zlib')\n",
    "    hdf.put('population',pdf,format='table')\n",
    "    hdf.close()\n",
    "    \n",
    "def load_population_h5(fname):\n",
    "    hdf =  pd.HDFStore(fname, mode='r')\n",
    "    res = hdf['population'].copy()\n",
    "    hdf.close()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def save_sample_h5(fname,trace,COORDS = None, filter_df = None):\n",
    "    odims = [d for d in trace.predictions.dims if d not in ['chain','draw','obs_idx']]\n",
    "    \n",
    "    if COORDS is None: # Recover them from trace (requires posterior be saved in same trace)\n",
    "        inds = trace.posterior.indexes\n",
    "        coords = { t: list(inds[t]) for t in inds if t not in ['chain','draw'] and '_dim_' not in t}\n",
    "        COORDS = { 'immutable': coords, 'mutable': ['obs_idx'] }\n",
    "\n",
    "    if filter_df is None: # Recover filter dimensions and data from trace (works only for GLMs)\n",
    "        rmdims = odims + list({'time','unit','combined_inputs'} & set(trace.predictions_constant_data.dims))\n",
    "        df = trace.predictions_constant_data.drop_dims(rmdims).to_dataframe()#.set_index(demographics_order).indexb\n",
    "        df.columns = [ s.removesuffix('_id') for s in df.columns]\n",
    "        df.drop(columns=[c for c in df.columns if c[:4]=='obs_'],inplace=True)\n",
    "\n",
    "        for d in df.columns:\n",
    "            if d in COORDS['immutable']:\n",
    "                fs = COORDS['immutable'][d]\n",
    "                df[d] = pd.Categorical(df[d].replace(dict(enumerate(fs))),fs)\n",
    "                if d in orders: df[d] = pd.Categorical(df[d],orders[d],ordered=True)\n",
    "        filter_df = df\n",
    "\n",
    "    chains, draws = trace.predictions.dims['chain'], trace.predictions.dims['draw']\n",
    "    dinds = np.array(list(it.product( range(chains), range(draws), list(filter_df.index)))).reshape( (-1, 3) )\n",
    "\n",
    "    res_dfs = { 'filter': filter_df }\n",
    "    for odim in odims:\n",
    "        response_cols = list(np.array(trace.predictions[odim]))\n",
    "        xdf = pd.DataFrame(np.concatenate( (\n",
    "            dinds,\n",
    "            np.array(trace.predictions['y_'+odim]).reshape( ( -1,len(response_cols) ) )\n",
    "            ), axis=-1), columns = ['chain', 'draw', 'obs_idx'] + response_cols)\n",
    "        res_dfs[odim] = postprocess_rdf(xdf,odim)\n",
    "        \n",
    "    # Save dfs as hdf5\n",
    "    hdf = pd.HDFStore(fname,complevel=9, complib='zlib')\n",
    "    for k,vdf in res_dfs.items():\n",
    "        hdf.put(k,vdf,format='table')\n",
    "    hdf.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# These two very helpful functions are borrowed from https://towardsdatascience.com/saving-metadata-with-dataframes-71f51f558d8e\n",
    "\n",
    "custom_meta_key = 'salk-toolkit-meta'\n",
    "\n",
    "def save_parquet_with_metadata(df, meta, file_name):\n",
    "    table = pa.Table.from_pandas(df)\n",
    "    \n",
    "    custom_meta_json = json.dumps(meta)\n",
    "    existing_meta = table.schema.metadata\n",
    "    combined_meta = {\n",
    "        custom_meta_key.encode() : custom_meta_json.encode(),\n",
    "        **existing_meta\n",
    "    }\n",
    "    table = table.replace_schema_metadata(combined_meta)\n",
    "    \n",
    "    pq.write_table(table, file_name, compression='GZIP')\n",
    "    \n",
    "def load_parquet_with_metadata(file_name,**kwargs):\n",
    "    restored_table = pq.read_table(file_name,**kwargs)\n",
    "    restored_df = restored_table.to_pandas()\n",
    "    if custom_meta_key.encode() in restored_table.schema.metadata:\n",
    "        restored_meta_json = restored_table.schema.metadata[custom_meta_key.encode()]\n",
    "        restored_meta = json.loads(restored_meta_json)\n",
    "    else: restored_meta = None\n",
    "    return restored_df, restored_meta\n",
    "\n",
    "# Just load the metadata from the parquet file\n",
    "# This is currently much more inefficient than it can be as it loads the entire table\n",
    "def load_parquet_metadata(file_name):\n",
    "    restored_table = pq.read_table(file_name)\n",
    "    if custom_meta_key.encode() in restored_table.schema.metadata:\n",
    "        restored_meta_json = restored_table.schema.metadata[custom_meta_key.encode()]\n",
    "        restored_meta = json.loads(restored_meta_json)\n",
    "    else: restored_meta = None\n",
    "    return restored_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test saving and loading parquet with metadata\n",
    "df = pd.DataFrame([[1,2],[3,4]],columns=['x','y'])\n",
    "meta = { 'test': [1,{'x':2},[3]] }\n",
    "\n",
    "save_parquet_with_metadata(df,meta,'test.parquet')\n",
    "ndf, nmeta = load_parquet_with_metadata('test.parquet')\n",
    "\n",
    "assert nmeta == meta\n",
    "assert ndf.equals(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is how you can add tests\n",
    "#assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
