{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils\n",
    "> Utility functions with wider use potential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "import json, os, warnings, math, inspect\n",
    "import itertools as it\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "import altair as alt\n",
    "import matplotlib.colors as mpc\n",
    "from copy import deepcopy\n",
    "from hashlib import sha256\n",
    "\n",
    "from typing import List, Tuple, Dict, Union, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# Value or Default - returns key value in dict if key in dict, otherwise Mone\n",
    "def vod(d,k,default=None): return d[k] if k in d else default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "\n",
    "# convenience for warnings that gives a more useful stack frame (fn calling the warning, not warning fn itself)\n",
    "warn = lambda msg,*args: warnings.warn(msg,*args,stacklevel=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# I'm surprised pandas does not have this function but I could not find it. \n",
    "def factorize_w_codes(s, codes):\n",
    "    res = s.replace(dict(zip(codes,range(len(codes)))))\n",
    "    if not s.isin(codes).all(): # Throw an exception if all values were not replaced\n",
    "        vals = set(s) - set(codes)\n",
    "        raise Exception(f'Codes for {s.name} do not match all values: {vals}')\n",
    "    return res.to_numpy(dtype='int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Simple batching of an iterable\n",
    "def batch(iterable, n=1):\n",
    "    l = len(iterable)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield iterable[ndx:min(ndx + n, l)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# turn index values into order indices\n",
    "def loc2iloc(index, vals):\n",
    "    d = dict(zip(np.array(index),range(len(index))))\n",
    "    return [ d[v] for v in vals ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Round in a way that preserves total sum\n",
    "def match_sum_round(s):\n",
    "    s = np.array(s)\n",
    "    fs = np.floor(s)\n",
    "    diff = round(s.sum()-fs.sum())\n",
    "    residues = np.argsort(-(s%1))[:diff]\n",
    "    fs[residues] = fs[residues]+1\n",
    "    return fs.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "assert (match_sum_round([0.7,0.7,0.6]) == [1,1,0]).all()\n",
    "assert (match_sum_round([1,2,3]) == [1,2,3]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Find the minimum difference between two values in the array\n",
    "def min_diff(arr):\n",
    "    b = np.diff(np.sort(arr))\n",
    "    if len(b)==0 or b.max()==0.0: return 0\n",
    "    else: return b[b>0].min()\n",
    "\n",
    "# Turn a discretized variable into a more smooth continuous one w a gaussian kernel\n",
    "def continify(ar, bounded=False):\n",
    "    mi,ma = ar.min(), ar.max()\n",
    "    noise = np.random.normal(0,0.5 * min_diff(ar),size=len(ar))\n",
    "    res = ar + noise\n",
    "    if bounded: # Reflect the noise on the boundaries\n",
    "        res[res>ma] = ma - (res[res>ma] - ma)\n",
    "        res[res<mi] = mi + (mi - res[res<mi])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "assert min_diff([0,0,2,1.5,3,3]) == 0.5\n",
    "assert min_diff([]) == min_diff([1,1]) == 0.0\n",
    "\n",
    "ar = np.array([0,2,4,1,5]+ [5]*10 + [-1]*10)\n",
    "c_ar = continify(ar,True)\n",
    "assert c_ar.min() >= ar.min() and c_ar.max()<=ar.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.optimize import linear_sum_assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Match data1 with data2 on columns cols as closely as possible\n",
    "def match_data(data1,data2,cols=None):\n",
    "    d1 = data1[cols].copy().dropna()\n",
    "    d2 = data2[cols].copy().dropna()\n",
    "\n",
    "    ccols = [c for c in cols if d1[c].dtype.name=='category']\n",
    "    for c in ccols: # replace categories with their index. This is ok for ordered categories, not so great otherwise\n",
    "        s1, s2 = set(d1[c].dtype.categories), set(d2[c].dtype.categories)\n",
    "        if s1-s2 and s2-s1: # one-way imbalance is fine\n",
    "            raise Exception(f\"Categorical columns differ in their categories on: {s1-s2} vs {s2-s1}\")\n",
    "        \n",
    "        md = d1 if len(s2-s1)==0 else d2\n",
    "        mdict = dict(zip(md[c].dtype.categories, range(len(md[c].dtype.categories))))\n",
    "        d1[c] = d1[c].replace(mdict)\n",
    "        d2[c] = d2[c].replace(mdict)\n",
    "\n",
    "    dmat = cdist(d1, d2, 'mahalanobis')\n",
    "    i1, i2 = linear_sum_assignment(dmat, maximize=False)\n",
    "    ind1, ind2 = d1.index[i1], d2.index[i2]\n",
    "    return ind1, ind2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "from salk_toolkit.io import process_annotated_data\n",
    "data = process_annotated_data('../data/master_meta.json')\n",
    "data1,data2 = data.iloc[:10], data.iloc[10:]\n",
    "cols = ['age','gender','education','nationality']\n",
    "\n",
    "# Make sure everything except age (cols[0]) gets exactly matched\n",
    "i1,i2 = match_data(data1,data2,cols)\n",
    "assert (data1.loc[i1,cols[1:]].reset_index(drop=True) == data2.loc[i2,cols[1:]].reset_index(drop=True)).all().all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Allow 'constants' entries in the dict to provide replacement mappings\n",
    "# This leads to much more readable jsons as repetitions can be avoided\n",
    "def replace_constants(d, constants = {}, inplace=False):\n",
    "    if not inplace: d = deepcopy(d)\n",
    "    if type(d)==dict and 'constants' in d:\n",
    "        constants = constants.copy() # Otherwise it would propagate back up through recursion - see test6 below\n",
    "        constants.update(d['constants'])\n",
    "        del d['constants']\n",
    "\n",
    "    for k, v in (d.items() if type(d)==dict else enumerate(d)):\n",
    "        if type(v)==str and v in constants:\n",
    "            d[k] = constants[v]\n",
    "        elif type(v)==dict or type(v)==list:\n",
    "            d[k] = replace_constants(v,constants, inplace=True)\n",
    "            \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "\n",
    "# Test replace_constants\n",
    "d = {\n",
    "    'constants': { 'a': {'a':1}, 'b':['b'] },\n",
    "    'test1': 'a',\n",
    "    'test2': [1,'b'],\n",
    "    'test3': { 'xy': 'a' },\n",
    "    'test4': { 'xy': [2, 'b'] },\n",
    "    'test5': { 'constants': {'a': ['a'] }, 'x':'a' },\n",
    "    'test6': 'a'\n",
    "}\n",
    "dr = replace_constants(d)\n",
    "assert dr == {'test1': {'a': 1}, 'test2': [1, ['b']], 'test3': {'xy': {'a': 1}}, 'test4': {'xy': [2, ['b']]}, 'test5': {'x': ['a']}, 'test6': {'a': 1}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# JSON encoder needed to convert pandas indices into lists for serialization\n",
    "def index_encoder(z):\n",
    "    if isinstance(z, pd.Index):\n",
    "        return list(z)\n",
    "    else:\n",
    "        type_name = z.__class__.__name__\n",
    "        raise TypeError(f\"Object of type {type_name} is not serializable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "default_color = 'lightgrey' # Something that stands out so it is easy to notice a missing color\n",
    "\n",
    "# Helper function to turn a dictionary into an Altair scale (or None into alt.Undefined)\n",
    "# Also: preserving order matters because scale order overrides sort argument\n",
    "def to_alt_scale(scale, order=None):\n",
    "    if scale is None: scale = alt.Undefined\n",
    "    if isinstance(scale,dict):\n",
    "        if order is None: order = scale.keys()\n",
    "        #else: order = [ c for c in order if c in scale ]\n",
    "        scale = alt.Scale(domain=list(order),range=[ (scale[c] if c in scale else default_color) for c in order ])\n",
    "    return scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Turn a question with multiple variants all of which are in distinct columns into a two columns - one with response, the other with which question variant was used\n",
    "\n",
    "def multicol_to_vals_cats(df, cols=None, col_prefix=None, reverse_cols=[], reverse_suffixes=None, cat_order=None, vals_name='vals', cats_name='cats', inplace=False):\n",
    "    if not inplace: df = df.copy()\n",
    "    if cols is None: cols = [ c for c in df.columns if c.startswith(col_prefix)]\n",
    "    \n",
    "    if not reverse_cols and reverse_suffixes is not None:\n",
    "        reverse_cols = list({ c for c in cols for rs in reverse_suffixes if c.endswith(rs)})\n",
    "    \n",
    "    if len(reverse_cols)>0:\n",
    "        #print(\"RC\",reverse_cols)\n",
    "        remap = dict(zip(cat_order,reversed(cat_order)))\n",
    "        df.loc[:,reverse_cols] = df.loc[:,reverse_cols].replace(remap)\n",
    "    \n",
    "    tdf = df[cols]\n",
    "    cinds = np.argmax(tdf.notna(),axis=1)\n",
    "    df.loc[:,vals_name] = np.array(tdf)[range(len(tdf)),cinds]\n",
    "    df.loc[:,cats_name] = np.array(tdf.columns)[cinds]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "\n",
    "df = pd.DataFrame({ 'q1': ['a','b','c',None,None,None], 'q1b': [None,None,None,'c','b','a'] })\n",
    "ndf = multicol_to_vals_cats(df,col_prefix='q1',reverse_suffixes=['1b'],cat_order=['a','b','c'])\n",
    "assert (ndf['vals'] == ['a','b','c','a','b','c']).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Grad is a list of colors\n",
    "def gradient_to_discrete_color_scale( grad, num_colors):\n",
    "    cmap = mpc.LinearSegmentedColormap.from_list('grad',grad)\n",
    "    return [mpc.to_hex(cmap(i)) for i in np.linspace(0, 1, num_colors)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert gradient_to_discrete_color_scale(['#ff0000','#ffff00','#00ff00'],4) == ['#ff0000', '#ffaa00', '#aaff00', '#00ff00']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def is_datetime(col):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "        return pd.api.types.is_datetime64_any_dtype(col) or (col.dtype.name in ['str','object'] and pd.to_datetime(col,errors='coerce').notna().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Convert a series of wave indices and a series of survey dates into a time series usable by our gp model\n",
    "def rel_wave_times(ws, dts, dt0=None):\n",
    "    df = pd.DataFrame({'wave':ws, 'dt': pd.to_datetime(dts)})\n",
    "    adf = df.groupby('wave')['dt'].median()\n",
    "    if dt0 is None: dt0 = adf.max() # use last wave date as the reference\n",
    "    \n",
    "    w_to_time = dict(((adf - dt0).dt.days/30).items())\n",
    "    \n",
    "    return pd.Series(df['wave'].replace(w_to_time),name='t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from salk_toolkit.io import process_annotated_data\n",
    "data = process_annotated_data('../data/master_meta.json')\n",
    "assert (rel_wave_times(data['wave'],data['date'])-data['t']).std() < 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Generate a random draws column that is deterministic in n, n_draws and uid\n",
    "def stable_draws(n, n_draws, uid):\n",
    "    # Initialize a random generator with a hash of uid\n",
    "    bgen = np.random.SFC64(np.frombuffer(sha256(str(uid).encode(\"utf-8\")).digest(), dtype='uint32'))\n",
    "    gen = np.random.Generator(bgen)\n",
    "    \n",
    "    n_samples = int(math.ceil(n/n_draws))\n",
    "    draws = (list(range(n_draws))*n_samples)[:n]\n",
    "    return gen.permuted(draws)\n",
    "\n",
    "# Use the stable_draws function to deterministicall assign shuffled draws to a df \n",
    "def deterministic_draws(df, n_draws, uid, n_total=None):\n",
    "    if n_total is None: n_total = len(df)\n",
    "    df.loc[:,'draw'] = pd.Series(stable_draws(n_total, n_draws, uid), index = np.arange(n_total))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (stable_draws(20,5,'test') == np.array([1, 2, 3, 3, 2, 3, 2, 2, 0, 0, 0, 3, 4, 4, 1, 1, 1, 0, 4, 4])).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Clean kwargs leaving only parameters fn can digest\n",
    "def clean_kwargs(fn, kwargs):\n",
    "    aspec = inspect.getfullargspec(fn)\n",
    "    return { k:v for k,v in kwargs.items() if k in aspec.args } if aspec.varkw is None else kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Simple one-liner to remove certain keys from a dict\n",
    "def censor_dict(d,vs):\n",
    "    return { k:v for k,v in d.items() if k not in vs }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# A nicer behaving wrapper around pd.cut\n",
    "def cut_nice(s, breaks, ints=True):\n",
    "    s = np.array(s)\n",
    "    \n",
    "    # Extend breaks if needed\n",
    "    if s.max()>breaks[-1]:\n",
    "        breaks += [s.max()+1]\n",
    "    if s.min()<breaks[0]:\n",
    "        breaks = [s.min()] + breaks\n",
    "    \n",
    "    labels = [ f'{breaks[i]} - {breaks[i+1] + (-1 if ints else 0)}' for i in range(len(breaks)-2) ] + [f'{breaks[-2]}+']\n",
    "    \n",
    "    return pd.cut(s,breaks,right=False,labels=labels)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (cut_nice([20,29,30,39,199],[0,20,30,40,50,60,70]) == ['20 - 29', '20 - 29', '30 - 39', '30 - 39', '70+']).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
